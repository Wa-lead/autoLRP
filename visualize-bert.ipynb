{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/waleed/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LRPLayer' from partially initialized module 'autoLRP.LRPnn' (most likely due to a circular import) (/Users/waleedalasad/Documents/GitHub/autoLRP/autoLRP/LRPnn.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoLRP\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLRPTensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRPTensor\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoLRP\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLRPnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRPModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Setup device\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/autoLRP/autoLRP/LRPnn.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_bert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertEncoder\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLRPBert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRPBertModel, LRPBertEncoder, BERTXAIConfig\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLRPTensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRPTensor\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLRPLayer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/Documents/GitHub/autoLRP/autoLRP/LRPBert.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutputWithPastAndCrossAttentions\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLRPTensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRPTensor\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLRPnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRPLayer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LRPLayer' from partially initialized module 'autoLRP.LRPnn' (most likely due to a circular import) (/Users/waleedalasad/Documents/GitHub/autoLRP/autoLRP/LRPnn.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from autoLRP.LRPTensor import LRPTensor\n",
    "from autoLRP.LRPnn import LRPModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup device\n",
    "device = \"mps\"\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# \n",
    "text = \"A woman is playing the guitar.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "fe = model.embeddings.word_embeddings(input_ids)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = LRPTensor(fe).to(device)\n",
    "lrp_model = LRPModel(model).to(device)\n",
    "he = lrp_model(inputs_embeds=fe).last_hidden_state.mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/waleed/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/waleed/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/waleed/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence 1: Is the following sentence plausible? 'LeBron James took a corner kick.'\n",
      "Decoded Sentence 1:  the best? be?\n",
      "Theaving James is a shot pass to\n",
      "\n",
      "Generated Response 1: \n",
      "\n",
      ",\n",
      ".\n",
      "\n",
      "\n",
      "Original Sentence 2: Is the following sentence plausible? 'LeBron James took a corner kick.' I think the answer is plausible, but I'm curious to hear what you think.\n",
      "Decoded Sentence 2:  the best? be?\n",
      "Theaving James is a shot pass to\n",
      " don it answer is yes. but I don not to see what the think.\n",
      "\n",
      "Generated Response 2: \n",
      "\n",
      ".\n",
      ",\n",
      " (\n",
      "-\n",
      " and the\n",
      " in the \"\n",
      " \", the, \" (, and, ( a, a\n",
      ":\n",
      " I. \" \" in a. ( \" and a and\n",
      "\n",
      "\n",
      "\n",
      "â€”\n",
      " [\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the smallest GPT2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    # Tokenize and encode the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Get the last hidden state (you can experiment with other layers)\n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    \n",
    "    return embeddings, inputs.input_ids\n",
    "\n",
    "def decode_embeddings(embeddings):\n",
    "    # Use the language model head to generate logits\n",
    "    logits = model.lm_head(embeddings)\n",
    "    \n",
    "    # Get the most likely token at each position\n",
    "    generated_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Decode the generated ids back to text\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def generate_response(embeddings):\n",
    "    # Use the embeddings as the initial hidden state\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs_embeds=embeddings,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    # Decode the generated ids back to text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"Is the following sentence plausible? 'LeBron James took a corner kick.'\"\n",
    "sentence2 = \"Is the following sentence plausible? 'LeBron James took a corner kick.' I think the answer is plausible, but I'm curious to hear what you think.\"\n",
    "\n",
    "# Encode sentences\n",
    "embeddings1, input_ids1 = encode_sentence(sentence1)\n",
    "embeddings2, input_ids2 = encode_sentence(sentence2)\n",
    "\n",
    "# Decode embeddings back to sentences\n",
    "decoded_sentence1 = decode_embeddings(embeddings1)\n",
    "decoded_sentence2 = decode_embeddings(embeddings2)\n",
    "\n",
    "# Generate responses based on embeddings\n",
    "response1 = generate_response(embeddings1)\n",
    "response2 = generate_response(embeddings2)\n",
    "\n",
    "print(\"Original Sentence 1:\", sentence1)\n",
    "print(\"Decoded Sentence 1:\", decoded_sentence1)\n",
    "print(\"Generated Response 1:\", response1)\n",
    "print(\"\\nOriginal Sentence 2:\", sentence2)\n",
    "print(\"Decoded Sentence 2:\", decoded_sentence2)\n",
    "print(\"Generated Response 2:\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence 1: Is the following sentence plausible? 'LeBron James took a corner kick.'\n",
      "Decoded Sentence 1:  the best? be?\n",
      "Theaving James is a shot pass to\n",
      "\n",
      "\n",
      "Original Sentence 2: Is the following sentence plausible? 'LeBron James took a corner kick.' I think the answer is plausible, but I'm curious to hear what you think.\n",
      "Decoded Sentence 2:  the best? be?\n",
      "Theaving James is a shot pass to\n",
      " don it answer is yes. but I don not to see what the think.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Check if 'mps' is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the smallest GPT2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    # Tokenize and encode the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Get the last hidden state (you can experiment with other layers)\n",
    "    embeddings = outputs.hidden_states[-1]\n",
    "    \n",
    "    return embeddings, inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "def decode_embeddings(embeddings, attention_mask):\n",
    "    # Use the language model head to generate logits\n",
    "    logits = model.lm_head(embeddings)\n",
    "    \n",
    "    # Get the most likely token at each position\n",
    "    generated_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Decode the generated ids back to text\n",
    "    generated_text = tokenizer.decode(generated_ids[0].cpu(), skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def generate_response(embeddings, attention_mask):\n",
    "    # Use the embeddings as the initial hidden state\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the generated ids back to text\n",
    "    generated_text = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"Is the following sentence plausible? 'LeBron James took a corner kick.'\"\n",
    "sentence2 = \"Is the following sentence plausible? 'LeBron James took a corner kick.' I think the answer is plausible, but I'm curious to hear what you think.\"\n",
    "\n",
    "# Encode sentences\n",
    "embeddings1, input_ids1, attention_mask1 = encode_sentence(sentence1)\n",
    "embeddings2, input_ids2, attention_mask2 = encode_sentence(sentence2)\n",
    "\n",
    "# Decode embeddings back to sentences\n",
    "decoded_sentence1 = decode_embeddings(embeddings1, attention_mask1)\n",
    "decoded_sentence2 = decode_embeddings(embeddings2, attention_mask2)\n",
    "\n",
    "# Generate responses based on embeddings\n",
    "response1 = generate_response(embeddings1, attention_mask1)\n",
    "response2 = generate_response(embeddings2, attention_mask2)\n",
    "\n",
    "print(\"Original Sentence 1:\", sentence1)\n",
    "print(\"Decoded Sentence 1:\", decoded_sentence1)\n",
    "print(\"Generated Response 1:\", response1)\n",
    "print(\"\\nOriginal Sentence 2:\", sentence2)\n",
    "print(\"Decoded Sentence 2:\", decoded_sentence2)\n",
    "print(\"Generated Response 2:\", response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence 1: Is the following sentence plausible? 'LeBron James took a corner kick.'\n",
      "Decoded Sentence 1: . is the following sentence plausible?'lebron james took a'kick. '.\n",
      "Generated Response 1: ., often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many,\n",
      "\n",
      "Original Sentence 2: Is the following sentence plausible? 'LeBron James took a corner kick.' I think the answer is plausible, but I'm curious to hear what you think.\n",
      "Decoded Sentence 2: . is the following sentence plausible?'lebron james took a corner kick.'i think the answer is plausible, but i. m curious to hear what you think..\n",
      "Generated Response 2: ., often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many, often many,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    # Tokenize and encode the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    # Get the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return embeddings, inputs.input_ids\n",
    "\n",
    "def decode_embeddings(embeddings):\n",
    "    # Use the language model head to generate logits\n",
    "    logits = bert_mlm.cls(embeddings)\n",
    "    \n",
    "    # Get the most likely token at each position\n",
    "    generated_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Decode the generated ids back to text\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def generate_response(embeddings):\n",
    "    # Use the embeddings to predict the next tokens\n",
    "    with torch.no_grad():\n",
    "        logits = bert_mlm.cls(embeddings)\n",
    "    \n",
    "    # Generate text using a simple greedy decoding strategy\n",
    "    generated_ids = []\n",
    "    for _ in range(50):  # Generate up to 50 tokens\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits).unsqueeze(0)\n",
    "        generated_ids.append(next_token.item())\n",
    "        \n",
    "        # Update embeddings with the new token\n",
    "        new_embeddings = bert_model(input_ids=next_token.unsqueeze(0)).last_hidden_state\n",
    "        embeddings = torch.cat([embeddings, new_embeddings], dim=1)\n",
    "        logits = bert_mlm.cls(embeddings)\n",
    "    \n",
    "    # Decode the generated ids back to text\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"Is the following sentence plausible? 'LeBron James took a corner kick.'\"\n",
    "sentence2 = \"Is the following sentence plausible? 'LeBron James took a corner kick.' I think the answer is plausible, but I'm curious to hear what you think.\"\n",
    "\n",
    "# Encode sentences\n",
    "embeddings1, input_ids1 = encode_sentence(sentence1)\n",
    "embeddings2, input_ids2 = encode_sentence(sentence2)\n",
    "\n",
    "# Decode embeddings back to sentences\n",
    "decoded_sentence1 = decode_embeddings(embeddings1)\n",
    "decoded_sentence2 = decode_embeddings(embeddings2)\n",
    "\n",
    "# Generate responses based on embeddings\n",
    "response1 = generate_response(embeddings1)\n",
    "response2 = generate_response(embeddings2)\n",
    "\n",
    "print(\"Original Sentence 1:\", sentence1)\n",
    "print(\"Decoded Sentence 1:\", decoded_sentence1)\n",
    "print(\"Generated Response 1:\", response1)\n",
    "print(\"\\nOriginal Sentence 2:\", sentence2)\n",
    "print(\"Decoded Sentence 2:\", decoded_sentence2)\n",
    "print(\"Generated Response 2:\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/waleed/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs ðŸš€ðŸš€. The training has started on 2023-09-01.\n",
      "A few words about the Llamas: they come with some interesting characteristics (the most notable is that there are more females than males). This means that you need at least two male and one female model in each layer for the same task â€” so it'll have an average size similar if not slightly bigger per layers compared when you use all models together as single one. For example, our 5*5 TINKER model had an averge of around ~7M parameters while having only four of them per layer instead of eight like other TRIP-based architectures would do...\n",
      "The pretrained weights were also optimized by running multiple reranking/repackaging experiments before finally converging towards those which yielded usable results; these are detailed below!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers \n",
    "import torch\n",
    "model = \"TinyLlama/TinyLlama_v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"mps\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs ðŸš€ðŸš€. The training has started on 2023-09-01.',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.5,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=500,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waleed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
